{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>An application of the deep_neural_net_1 script for predicting fake news headlines.</h1>\n",
    "<br>\n",
    "<li>The training set contains 2000 training examples.</li>\n",
    "<li>The dev set contains 500 examples.</li>\n",
    "\n",
    "<p>By <a href='https://github.com/hrushikeshrv'>hrushikeshrv</a> on GitHub.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help ():\n",
    "    \"\"\"\n",
    "    Returns some helpful information on how to use\n",
    "    \"\"\"\n",
    "    print('#'*5 + '-'*100 + '#'*5)\n",
    "    print('\\n\\n')\n",
    "    print('You will need to decide on the following hyperparameters to initialize your model - \\n\\n')\n",
    "    print('The architecture of your model. \\n\\t Decide on the number of layers your network will have, and the number of units in each of these layers. \\n')\n",
    "    print('The activation functions to use in each layer. \\n\\t Most often you will want to use a ReLU activation for the hidden units and a sigmoid or softmax activation for the output.\\n')\n",
    "    print('The learning rate alpha of your model. \\n\\t Use a small number like 0.01, but not too small or gradient descent won\\'t converge on the global minima. If you use too big a value, gradient descent can start to diverge.\\n')\n",
    "    print('The number of iterations of gradient descent you want to run. \\n\\t This model currently only supports gradient descent as an optimization algorithm, but I will be adding other optimizers like ADAM, RMS Prop, and Momentum soon.\\n')\n",
    "    print('\\n')\n",
    "    print('#'*5 + '-'*100 + '#'*5)\n",
    "    print('\\n\\nIf you\\'re working locally, make sure you have numpy and matplotlib installed.')\n",
    "    print('\\n')\n",
    "    print('To build a pre-implemented logistic or softmax network, call the \\'four_layer_logistic()\\' or \\'five_layer_logistic()\\' functions and pass them the parameters they need.\\n\\n')\n",
    "    print('To build a general network, simply call the model() function and pass it the parameters it needs. Have your input data and your output labels prepared and formatted how you want before you initialize.')\n",
    "    print('Calling the model() function will begin training your given network on your given dataset for a default of 10,000 iterations\\n')\n",
    "    print('It will print the cost and the training accuracy of your model after it is done training.\\n\\n')\n",
    "    print('#'*5 + '-'*100 + '#'*5)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid (Z):\n",
    "    \"\"\"\n",
    "    Takes in a real number or matrix Z and applies the sigmoid function to it.\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-1*Z))\n",
    "    return A\n",
    "\n",
    "def relu (Z):\n",
    "    \"\"\"\n",
    "    Takes in a real number or matrix Z and applies the (leaky) relu function to it.\n",
    "    \"\"\"\n",
    "    A = np.zeros(Z.shape)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            if Z[i,j] >= 0:\n",
    "                A[i,j] = Z[i,j]\n",
    "            else:\n",
    "                A[i,j] = 0.001*Z[i,j]\n",
    "    return A\n",
    "\n",
    "#NOTE ---- THIS DEFINITION MIGHT CAUSE BROADCASTING PROBLEMS DURING RUNTIME\n",
    "#NOTE 2.0 --- Probably fixed it.\n",
    "def drelu (Z):\n",
    "    \"\"\"\n",
    "    Takes in a real number Z and returns the derivative of the relu function at that value or matrix.\n",
    "    Used for back propagation. Z will be the activation value of the last layer before the non-linearity is applied.\n",
    "    \"\"\"\n",
    "    A = np.zeros(Z.shape)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            if Z[i,j] >= 0:\n",
    "                A[i,j] = 1\n",
    "            else:\n",
    "                A[i,j] = 0.001\n",
    "    return A\n",
    "\n",
    "def softmax (Z):\n",
    "    \"\"\"\n",
    "    Takes in a vector Z and returns its softmax activation.\n",
    "    \"\"\"\n",
    "    temp = np.exp(Z)\n",
    "    factor = np.sum(temp)\n",
    "    A = temp/factor\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters (architecture = []):\n",
    "    \"\"\"\n",
    "    Initializes the parameters of the neural net.\n",
    "    \n",
    "    Takes in the architecture of the network as a list.\n",
    "    Structure the list as the number of units in each layer, starting from the number of input features, to the number of output units.\n",
    "    [10, 5, 5, 4, 3] means that there are 10 input features, 3 hidden layers with 5, 5, 4 hidden units respectively, and 3 output units (softmax regression)\n",
    "\n",
    "    Returns a dictionary of keys W(i) for i from 1 to number of layers, and b(i) for the same i.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "\n",
    "    number_of_layers = len(architecture) - 1\n",
    "\n",
    "    for i in range(1,number_of_layers+1):\n",
    "\n",
    "        parameters['W' + str(i)] = np.random.randn(architecture[i], architecture[i-1])*0.01\n",
    "        parameters['b' + str(i)] = np.zeros((architecture[i], 1))\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation (X, parameters, num_layers, activation_func = []):\n",
    "    \"\"\"\n",
    "    Performs forward propagation\n",
    "\n",
    "    Takes in the activations of each layer and the parameters.\n",
    "    activation_func is a list with as many elements (strings - either relu, softmax, or sigmoid) as number of layers (excluding the input layer)\n",
    "    num_layers is the number of layers\n",
    "    parameters is a dictionary containing all the W and b values for all the layers.\n",
    "    X is the input\n",
    "\n",
    "    Returns the final prediction y_hat and all the Z and A values as cache to use for backward propagation.\n",
    "    \"\"\"\n",
    "    activations = {'A0': X}\n",
    "\n",
    "    for i in range(1, num_layers+1):\n",
    "        W = parameters['W' + str(i)]\n",
    "        b = parameters['b' + str(i)]\n",
    "\n",
    "        activations['Z' + str(i)] = np.dot(W, activations['A' + str(i-1)]) + b\n",
    "        \n",
    "        if activation_func[i-1].lower() == 'relu':\n",
    "            activations['A' + str(i)] = relu(activations['Z' + str(i)])\n",
    "\n",
    "        elif activation_func[i-1].lower() == 'softmax':\n",
    "            activations['A' + str(i)] = softmax(activations['Z' + str(i)])\n",
    "\n",
    "        elif activation_func[i-1].lower() == 'sigmoid':\n",
    "            activations['A' + str(i)] = sigmoid(activations['Z' + str(i)])\n",
    "        \n",
    "        activations['W' + str(i)] = W\n",
    "        activations['b' + str(i)] = b\n",
    "\n",
    "    return activations['A' + str(num_layers)], activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost (Y, prediction, activation_output = 'sigmoid'):\n",
    "    \"\"\"\n",
    "    Calculates the cost.\n",
    "\n",
    "    Takes in the output labels and the prediction from forward propagation, as well as the activation function of the output layer.\n",
    "    Returns the cost.\n",
    "\n",
    "    The inputs will be real numbers or row vectors of the same dimensions if the activation function of the last layer is sigmoid.\n",
    "    The inputs will be row vectors or row matrices of the same dimensions if the activation function of the last layer is softmax.\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    #------------------------------DEBUG------------------------------#\n",
    "    #print(f'The prediction A is {prediction}, the label Y is {Y}')\n",
    "    #------------------------------DEBUG------------------------------#\n",
    "\n",
    "    if activation_output.lower() == 'sigmoid':\n",
    "        cost = (-1/m)*np.sum(Y*np.log(prediction) + (1-Y)*np.log(1-prediction))\n",
    "\n",
    "    if activation_output.lower() == 'softmax':\n",
    "        cost = (-1/m)*np.sum(np.sum(Y*np.log(prediction), axis = 0, keepdims = True))\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation (X, Y, cache, number_of_layers):\n",
    "    \"\"\"\n",
    "    Performs backward propagation.\n",
    "\n",
    "    Takes in the inputs X, the corresponding labels Y, and the activation values stored as cache (returned by the second output for forward_propagation)\n",
    "    Returns the gradients of all parameters in the grads dictionary.\n",
    "\n",
    "    The cache is a dictionary containing the Z values and the A values for all layers.\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    last_layer = 'dZ' + str(number_of_layers)\n",
    "    gradients = {last_layer: cache['A'+str(number_of_layers)]-Y}\n",
    "    \n",
    "    #------------------------------DEBUG------------------------------#\n",
    "#     print(f'Number of layers is {number_of_layers}. The gradients dictionary is = \\n {gradients}')\n",
    "#     print(f'Cache has - {cache.keys()}')\n",
    "#     print(f'Shape of A4 is {cache[\"A\" + str(number_of_layers)].shape}')\n",
    "#     print(f'Shape of dZ4 is {gradients[last_layer].shape}')\n",
    "#     print(f'Shape of Y is {Y.shape}')\n",
    "#     print(f'Shape of X is {X.shape}')\n",
    "    #------------------------------DEBUG------------------------------#\n",
    "\n",
    "    for i in reversed(range(2,number_of_layers+1)):\n",
    "        gradients['dW' + str(i)] = (1/m)*np.dot(gradients['dZ'+str(i)], cache['A' + str(i-1)].T)\n",
    "        gradients['db' + str(i)] = (1/m)*np.sum(gradients['dZ'+str(i)], axis = 1, keepdims = True)\n",
    "        \n",
    "        #------------------------------DEBUG------------------------------#\n",
    "#         print(f'Shape of W{i} = {cache[\"W\" + str(i)].T.shape} \\nShape of dZ{i} = {gradients[\"dZ\" + str(i)].shape}')\n",
    "        #------------------------------DEBUG------------------------------#\n",
    "        \n",
    "        gradients['dZ' + str(i-1)] = np.dot(cache['W' + str(i)].T, gradients['dZ' + str(i)])*drelu(cache['Z' + str(i-1)])\n",
    "    \n",
    "    gradients['dW1'] = (1/m)*np.dot(gradients['dZ'+str(1)], cache['A' + str(0)].T)\n",
    "    gradients['db1'] = (1/m)*np.sum(gradients['dZ'+str(1)], axis = 1, keepdims = True)\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters (parameters, number_of_layers, gradients, alpha = 0.001):\n",
    "    \"\"\"\n",
    "    Updates the parameters.\n",
    "\n",
    "    Takes in the parameters themselves (as the parameters dictionary returned by the initialize_parameters function dictionary),\n",
    "    the gradients (as the gradients dictionary returned by the backward_propagation function), and the learning rate alpha.\n",
    "    Returns the updated parameters.\n",
    "    \"\"\"\n",
    "    for i in range(1, number_of_layers+1):\n",
    "        parameters['W' + str(i)] = parameters['W' + str(i)] - alpha*gradients['dW' + str(i)]\n",
    "        parameters['b' + str(i)] = parameters['b' + str(i)] - alpha*gradients['db' + str(i)]\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy (X, Y, parameters, number_of_layers, activation_functions):\n",
    "    \"\"\"\n",
    "    Runs forward propagation on all examples and returns the accuracy of the model\n",
    "\n",
    "    Takes in X and Y, the parameters, number of layers, and the activation functions list. Meant to be run inside the model() function definition.\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    correct_count = 0\n",
    "\n",
    "    # for i in range(m):\n",
    "    #     X_in = X[:, i]\n",
    "    #     Y_out = Y[:, i]\n",
    "\n",
    "    #     pred = forward_propagation(X_in, parameters, number_of_layers, activation_functions)\n",
    "\n",
    "    #     if pred == Y_out:\n",
    "    #         correct_count += 1\n",
    "\n",
    "    pred,_ = forward_propagation(X, parameters, number_of_layers, activation_functions)\n",
    "    for i in range(m):\n",
    "        \n",
    "        #------------------------------DEBUG------------------------------#\n",
    "#         print(f'The prediction of the model is - {pred[:,i]}. The true label is - {Y[:, i]}')\n",
    "#         print(pred[:,i].shape)\n",
    "#         print(pred[:,i])\n",
    "#         print(Y[:,i].shape)\n",
    "#         print(Y[:,i])\n",
    "        #------------------------------DEBUG------------------------------#\n",
    "        \n",
    "        if (pred[:, i] >= 0.5 and Y[:, i] == 1) or (pred[:,i] < 0.5 and Y[:,i] == 0):\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count*100/m\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (X, Y, architecture, activation_functions, learning_rate = 0.001, print_cost = True, number_of_iterations = 10000):\n",
    "    \"\"\"\n",
    "    Takes in the training set X, the labels Y, and all the required parameters and trains the defined model for the given number of iterations.\n",
    "    Prints the cost if print_cost is true.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    number_of_layers = len(architecture) - 1\n",
    "\n",
    "    parameters = initialize_parameters(architecture)\n",
    "\n",
    "    for i in range(number_of_iterations):\n",
    "\n",
    "        prediction, cache = forward_propagation(X, parameters, number_of_layers, activation_functions)\n",
    "        cost = calculate_cost(Y, prediction, activation_functions[-1])\n",
    "\n",
    "        costs.append(cost)\n",
    "        gradients = backward_propagation(X, Y, cache, number_of_layers)\n",
    "\n",
    "        parameters = update_parameters(parameters, number_of_layers, gradients, alpha=0.01)\n",
    "\n",
    "        if print_cost and i%500 == 0:\n",
    "            print(f'Completed {i} iterations.')\n",
    "            print(f'Cost after iteration {i} = {cost}\\n')\n",
    "    \n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('Iterations (in hundreds)')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title(f'Learning rate = {learning_rate}')\n",
    "    plt.show()\n",
    "    \n",
    "    costs_df = pd.DataFrame(costs)\n",
    "    sns.set(style = 'whitegrid')\n",
    "    # sns.set_context(context = 'talk')\n",
    "    plt.figure(figsize = (10,5))\n",
    "    sns.lineplot(data = costs_df, palette = 'magma', linewidth = 3)\n",
    "\n",
    "    print(f'Ran {number_of_iterations} iterations. Returning parameters now.')\n",
    "    print(f'Parameters returned - {parameters.keys()}')\n",
    "    print(f'The final cost of the model after training was {costs[-1]}')\n",
    "    acc = calculate_accuracy(X, Y, parameters, number_of_layers, activation_functions)\n",
    "    print(f'The training accuracy was: {acc}%')\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_layer_logistic (X, Y, architecture =  [10, 5, 5, 1], activation_functions = ['relu', 'relu', 'relu', 'relu', 'sigmoid'], learning_rate = 0.001, print_cost = True, number_of_iterations = 10000):\n",
    "    \"\"\"\n",
    "    Predefined function to construct a four layer (logistic) plain neural network. \n",
    "    If you want to override the architecture to construct a four layer softmax network, you can overwrite the values of the default parameters 'architecture' and 'activation_functions'.\n",
    "\n",
    "    Takes in only the input X and the output labels Y.\n",
    "    \"\"\"\n",
    "    temp = X.shape[0]\n",
    "    architecture.insert(0, temp)\n",
    "    tic = time.time()\n",
    "    p = model(X, Y, architecture, activation_functions, learning_rate, print_cost, number_of_iterations)\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(f'Took {toc - tic} seconds to train.')\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_layer_logistic (X, Y, architecture = [20, 10, 5, 5, 1], activation_functions = ['relu', 'relu', 'relu', 'relu', 'relu', 'sigmoid'], learning_rate = 0.001, print_cost = True, number_of_iterations = 10000):\n",
    "    \"\"\"\n",
    "    Predifined function to construct a five layer logistic plain neural network.\n",
    "    If you want to override the architecture to construct a five layer softmax network, you can overwrite the values of the default parameters 'architecture' and 'activation_functions'.\n",
    "\n",
    "    Takes in only the input X and the output labels Y.\n",
    "    \"\"\"\n",
    "    temp = X.shape[0]\n",
    "    architecture.insert(0, temp)\n",
    "    tic = time.time()\n",
    "    p = model(X, Y, architecture, activation_functions, learning_rate, print_cost, number_of_iterations)\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(f'Took {toc - tic} seconds to train.')\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_accuracy (X, Y, parameters, architecture, activation_functions):\n",
    "    \"\"\"\n",
    "    Takes in the dev set examples and dev set labels along with previously trained parameters and prints the accuracy.\n",
    "    \"\"\"\n",
    "    number_of_layers = len(architecture) - 1\n",
    "    pred,_ = forward_propagation(X, parameters, number_of_layers+1, activation_functions)\n",
    "    \n",
    "    correct_count = 0\n",
    "    for i in range(pred.shape[1]):\n",
    "        if (pred[:,i] >= 0.5 and Y[:,i] == 1) or (pred[:,i] < 0.5 and Y[:,i] == 0):\n",
    "            correct_count += 1\n",
    "    \n",
    "    acc = correct_count*100/pred.shape[1]\n",
    "    print(f'The development set accuracy was {acc}%')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------#\n",
    "#-----------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def test_accuracy (X, Y, parameters, architecture, activation_functions):\n",
    "    \"\"\"\n",
    "    Takes in the dev set examples and dev set labels along with previously trained parameters and prints the accuracy.\n",
    "    \"\"\"\n",
    "    number_of_layers = len(architecture) - 1\n",
    "    pred,_ = forward_propagation(X, parameters, number_of_layers+1, activation_functions)\n",
    "    \n",
    "    correct_count = 0\n",
    "    for i in range(pred.shape[1]):\n",
    "        if (pred[:,i] >= 0.5 and Y[:,i] == 1) or (pred[:,i] < 0.5 and Y[:,i] == 0):\n",
    "            correct_count += 1\n",
    "    \n",
    "    acc = correct_count*100/pred.shape[1]\n",
    "    print(f'The test set accuracy was {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('training_set.csv', header = None).to_numpy()\n",
    "Y = pd.read_csv('training_labels.csv', header = None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = X[:, :2000]\n",
    "Y_train_1 = Y[:2000, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 iterations.\n",
      "Cost after iteration 0 = nan\n",
      "\n",
      "Completed 100 iterations.\n",
      "Cost after iteration 100 = 0.6796306185368487\n",
      "\n",
      "Completed 200 iterations.\n",
      "Cost after iteration 200 = 0.6439346894386155\n",
      "\n",
      "Completed 300 iterations.\n",
      "Cost after iteration 300 = 0.6396983228259713\n",
      "\n",
      "Completed 400 iterations.\n",
      "Cost after iteration 400 = 0.6391147293620209\n",
      "\n",
      "Completed 500 iterations.\n",
      "Cost after iteration 500 = 0.6390140756132754\n",
      "\n",
      "Completed 600 iterations.\n",
      "Cost after iteration 600 = 0.6389813569619515\n",
      "\n",
      "Completed 700 iterations.\n",
      "Cost after iteration 700 = 0.6389586669776806\n",
      "\n",
      "Completed 800 iterations.\n",
      "Cost after iteration 800 = 0.6389361545286558\n",
      "\n",
      "Completed 900 iterations.\n",
      "Cost after iteration 900 = 0.6389115855885152\n",
      "\n",
      "Completed 1000 iterations.\n",
      "Cost after iteration 1000 = 0.6388841086457613\n",
      "\n",
      "Completed 1100 iterations.\n",
      "Cost after iteration 1100 = 0.6388525497055418\n",
      "\n",
      "Completed 1200 iterations.\n",
      "Cost after iteration 1200 = 0.6388156201525247\n",
      "\n",
      "Completed 1300 iterations.\n",
      "Cost after iteration 1300 = 0.6387718609258282\n",
      "\n",
      "Completed 1400 iterations.\n",
      "Cost after iteration 1400 = 0.6387197508249488\n",
      "\n",
      "Completed 1500 iterations.\n",
      "Cost after iteration 1500 = 0.6386572909480039\n",
      "\n",
      "Completed 1600 iterations.\n",
      "Cost after iteration 1600 = 0.6385818769973389\n",
      "\n",
      "Completed 1700 iterations.\n",
      "Cost after iteration 1700 = 0.6384904624977971\n",
      "\n",
      "Completed 1800 iterations.\n",
      "Cost after iteration 1800 = 0.638381533039963\n",
      "\n",
      "Completed 1900 iterations.\n",
      "Cost after iteration 1900 = 0.6382515925713603\n",
      "\n",
      "Completed 2000 iterations.\n",
      "Cost after iteration 2000 = 0.638094030439607\n",
      "\n",
      "Completed 2100 iterations.\n",
      "Cost after iteration 2100 = 0.6379023171879629\n",
      "\n",
      "Completed 2200 iterations.\n",
      "Cost after iteration 2200 = 0.6376682859342004\n",
      "\n",
      "Completed 2300 iterations.\n",
      "Cost after iteration 2300 = 0.6373806838113778\n",
      "\n",
      "Completed 2400 iterations.\n",
      "Cost after iteration 2400 = 0.6370249905113246\n",
      "\n",
      "Completed 2500 iterations.\n",
      "Cost after iteration 2500 = 0.6365836791532622\n",
      "\n",
      "Completed 2600 iterations.\n",
      "Cost after iteration 2600 = 0.6360292977261666\n",
      "\n",
      "Completed 2700 iterations.\n",
      "Cost after iteration 2700 = 0.6353243329752188\n",
      "\n",
      "Completed 2800 iterations.\n",
      "Cost after iteration 2800 = 0.6344109471249747\n",
      "\n",
      "Completed 2900 iterations.\n",
      "Cost after iteration 2900 = 0.6332467931807384\n",
      "\n",
      "Completed 3000 iterations.\n",
      "Cost after iteration 3000 = 0.6317686937541744\n",
      "\n",
      "Completed 3100 iterations.\n",
      "Cost after iteration 3100 = 0.6298148814789482\n",
      "\n",
      "Completed 3200 iterations.\n",
      "Cost after iteration 3200 = 0.6271667517851308\n",
      "\n",
      "Completed 3300 iterations.\n",
      "Cost after iteration 3300 = 0.622971324716076\n",
      "\n",
      "Completed 3400 iterations.\n",
      "Cost after iteration 3400 = 0.6146136194026012\n",
      "\n",
      "Completed 3500 iterations.\n",
      "Cost after iteration 3500 = 0.593484488345659\n",
      "\n",
      "Completed 3600 iterations.\n",
      "Cost after iteration 3600 = 0.5548951284631679\n",
      "\n",
      "Completed 3700 iterations.\n",
      "Cost after iteration 3700 = 0.5167279220896981\n",
      "\n",
      "Completed 3800 iterations.\n",
      "Cost after iteration 3800 = 0.47168323181318783\n",
      "\n",
      "Completed 3900 iterations.\n",
      "Cost after iteration 3900 = 0.43060219112338843\n",
      "\n",
      "Completed 4000 iterations.\n",
      "Cost after iteration 4000 = 0.40150537932962255\n",
      "\n",
      "Completed 4100 iterations.\n",
      "Cost after iteration 4100 = 0.3594634043880623\n",
      "\n",
      "Completed 4200 iterations.\n",
      "Cost after iteration 4200 = 0.3478548027388864\n",
      "\n",
      "Completed 4300 iterations.\n",
      "Cost after iteration 4300 = 0.32304475128919385\n",
      "\n",
      "Completed 4400 iterations.\n",
      "Cost after iteration 4400 = nan\n",
      "\n",
      "Completed 4500 iterations.\n",
      "Cost after iteration 4500 = nan\n",
      "\n",
      "Completed 4600 iterations.\n",
      "Cost after iteration 4600 = nan\n",
      "\n",
      "Completed 4700 iterations.\n",
      "Cost after iteration 4700 = nan\n",
      "\n",
      "Completed 4800 iterations.\n",
      "Cost after iteration 4800 = nan\n",
      "\n",
      "Completed 4900 iterations.\n",
      "Cost after iteration 4900 = nan\n",
      "\n",
      "Completed 5000 iterations.\n",
      "Cost after iteration 5000 = nan\n",
      "\n",
      "Completed 5100 iterations.\n",
      "Cost after iteration 5100 = nan\n",
      "\n",
      "Completed 5200 iterations.\n",
      "Cost after iteration 5200 = nan\n",
      "\n",
      "Completed 5300 iterations.\n",
      "Cost after iteration 5300 = nan\n",
      "\n",
      "Completed 5400 iterations.\n",
      "Cost after iteration 5400 = nan\n",
      "\n",
      "Completed 5500 iterations.\n",
      "Cost after iteration 5500 = nan\n",
      "\n",
      "Completed 5600 iterations.\n",
      "Cost after iteration 5600 = nan\n",
      "\n",
      "Completed 5700 iterations.\n",
      "Cost after iteration 5700 = nan\n",
      "\n",
      "Completed 5800 iterations.\n",
      "Cost after iteration 5800 = nan\n",
      "\n",
      "Completed 5900 iterations.\n",
      "Cost after iteration 5900 = nan\n",
      "\n",
      "Completed 6000 iterations.\n",
      "Cost after iteration 6000 = nan\n",
      "\n",
      "Completed 6100 iterations.\n",
      "Cost after iteration 6100 = nan\n",
      "\n",
      "Completed 6200 iterations.\n",
      "Cost after iteration 6200 = nan\n",
      "\n",
      "Completed 6300 iterations.\n",
      "Cost after iteration 6300 = nan\n",
      "\n",
      "Completed 6400 iterations.\n",
      "Cost after iteration 6400 = nan\n",
      "\n",
      "Completed 6500 iterations.\n",
      "Cost after iteration 6500 = nan\n",
      "\n",
      "Completed 6600 iterations.\n",
      "Cost after iteration 6600 = nan\n",
      "\n",
      "Completed 6700 iterations.\n",
      "Cost after iteration 6700 = nan\n",
      "\n",
      "Completed 6800 iterations.\n",
      "Cost after iteration 6800 = nan\n",
      "\n",
      "Completed 6900 iterations.\n",
      "Cost after iteration 6900 = nan\n",
      "\n",
      "Completed 7000 iterations.\n",
      "Cost after iteration 7000 = nan\n",
      "\n",
      "Completed 7100 iterations.\n",
      "Cost after iteration 7100 = nan\n",
      "\n",
      "Completed 7200 iterations.\n",
      "Cost after iteration 7200 = nan\n",
      "\n",
      "Completed 7300 iterations.\n",
      "Cost after iteration 7300 = nan\n",
      "\n",
      "Completed 7400 iterations.\n",
      "Cost after iteration 7400 = nan\n",
      "\n",
      "Completed 7500 iterations.\n",
      "Cost after iteration 7500 = nan\n",
      "\n",
      "Completed 7600 iterations.\n",
      "Cost after iteration 7600 = nan\n",
      "\n",
      "Completed 7700 iterations.\n",
      "Cost after iteration 7700 = nan\n",
      "\n",
      "Completed 7800 iterations.\n",
      "Cost after iteration 7800 = nan\n",
      "\n",
      "Completed 7900 iterations.\n",
      "Cost after iteration 7900 = nan\n",
      "\n",
      "Completed 8000 iterations.\n",
      "Cost after iteration 8000 = nan\n",
      "\n",
      "Completed 8100 iterations.\n",
      "Cost after iteration 8100 = nan\n",
      "\n",
      "Completed 8200 iterations.\n",
      "Cost after iteration 8200 = nan\n",
      "\n",
      "Completed 8300 iterations.\n",
      "Cost after iteration 8300 = nan\n",
      "\n",
      "Completed 8400 iterations.\n",
      "Cost after iteration 8400 = nan\n",
      "\n",
      "Completed 8500 iterations.\n",
      "Cost after iteration 8500 = nan\n",
      "\n",
      "Completed 8600 iterations.\n",
      "Cost after iteration 8600 = nan\n",
      "\n",
      "Completed 8700 iterations.\n",
      "Cost after iteration 8700 = nan\n",
      "\n",
      "Completed 8800 iterations.\n",
      "Cost after iteration 8800 = nan\n",
      "\n",
      "Completed 8900 iterations.\n",
      "Cost after iteration 8900 = nan\n",
      "\n",
      "Completed 9000 iterations.\n",
      "Cost after iteration 9000 = nan\n",
      "\n",
      "Completed 9100 iterations.\n",
      "Cost after iteration 9100 = nan\n",
      "\n",
      "Completed 9200 iterations.\n",
      "Cost after iteration 9200 = nan\n",
      "\n",
      "Completed 9300 iterations.\n",
      "Cost after iteration 9300 = nan\n",
      "\n",
      "Completed 9400 iterations.\n",
      "Cost after iteration 9400 = nan\n",
      "\n",
      "Completed 9500 iterations.\n",
      "Cost after iteration 9500 = nan\n",
      "\n",
      "Completed 9600 iterations.\n",
      "Cost after iteration 9600 = nan\n",
      "\n",
      "Completed 9700 iterations.\n",
      "Cost after iteration 9700 = nan\n",
      "\n",
      "Completed 9800 iterations.\n",
      "Cost after iteration 9800 = nan\n",
      "\n",
      "Completed 9900 iterations.\n",
      "Cost after iteration 9900 = nan\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 10000 iterations. Returning parameters now.\n",
      "Parameters returned - dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4'])\n",
      "The final cost of the model after training was nan\n",
      "The training accuracy was: 99.7%\n",
      "Took 770.858943939209 seconds to train.\n",
      "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "four_layer_parameters = four_layer_logistic(X_train_1, Y_train_1)\n",
    "print(four_layer_parameters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# with open('four_layer_parameters.csv', 'w') as f:\n",
    "#     for key in four_layer_parameters.keys():\n",
    "#         f.write(\"%s,%s\\n\"%(key,four_layer_parameters[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_1 = X[:, 3000:3500]\n",
    "Y_dev_1 = Y[3000:3500, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The development set accuracy was 79.4%\n"
     ]
    }
   ],
   "source": [
    "dev_accuracy(X_dev_1, Y_dev_1, four_layer_parameters, architecture = [10, 5, 5, 1], activation_functions = ['relu', 'relu', 'relu', 'relu', 'sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
